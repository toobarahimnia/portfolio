<!DOCTYPE html>
<html lang="en">
    <head><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css">
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>DevJane Portfolio Website</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css" integrity="sha256-46qynGAkLSFpVbEBog43gvNhfrOj+BmwXdxFgVK/Kvc=" crossorigin="anonymous" />  
        
        <!-- Update these with your own fonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700|Roboto+Slab:400,700&display=swap" rel="stylesheet"> 
        
        <link rel="stylesheet" href="css/style.css">

    </head>
    <body>
        <header>
            <button class="nav-toggle" aria-label="toggle navigation">
                <span class="hamburger"></span>
            </button>
            <nav class="nav">
                <ul class="nav__list">
                    <li class="nav__item"><a href="index.html" class="nav__link">Home</a></li>
                    <li class="nav__item"><a href="index.html#about_me" class="nav__link">About me</a></li>
                    <li class="nav__item"><a href="index.html#my_working" class="nav__link">Work experience</a></li>
                    <li class="nav__item"><a href="index.html#projects" class="nav__link">Projects</a></li>
                </ul>
            </nav>
        </header>
        

        <section class="intro">
            <h1 class="section__title section__title--intro">
                <font size="12rem">Long short-term memory (LSTM)</font>
            </h1>
            <p class="section__subtitle section__subtitle--intro">Academic Project</p>
            <img src="img/LSTM.png" alt="" class="intro__img">
        </section>
        
        <div class="portfolio-item-individual"> 
          
            <!-- <div> 
                <div class="column">
                    <img src="img/deepdive2.jpg" alt="" style="width:450px;height:200px;">
                </div>
                <div class="column">
                    <img src="img/deepdive3.jpg" alt="" style="width:450px;height:200px;">
                </div>
                <div class="column">
                    <img src="img/deepdive4.jpg" alt="" style="width:450px;height:200px;">
                </div>
            </div> -->
        
            <p><strong>Introduction:</strong></p>
            <p>For this assignment we were tasked with creating weather forecasting deep models applied to the weather  time series dataset. These models took on three flavours: a model creating using only dense layers, a model involving LSTM layers, and finally, a custom model for which we chose to implement a transformer model. The first two models served to provide baselines for performance, whereas the final model was created in an attempt to outperform these baselines.  Each of the models received inputs in the form a history of weather attributes (discussed in section 2.1) from time t − k to t − 1 and output the following values for a time t:</p>
            <p><strong>p (mbar)</strong>  Atmospheric pressure<br><strong>T (degC)</strong>  Air temperature<strong><br>rh (%)</strong>  Relative humidity<br><strong>wv (m/s)</strong>   Wind velocity</p>
            <p>For the purposes of our experiments, we chose k = 7, as we believed that providing the models with a longer time series would likely lead to better inference by each model, however, wanted to balance this goal with reasonable training times, motivating us to have k be less than it’s maximum allowed value of 8. Additionally, each model consisted of exactly three deep learning layers to ensure fairness of comparison across models.</p>
           
            <!-- <p><ul style="float: right; margin-right: 100px;>
                </ul></p> -->
            <p><strong>Methodology</strong></p>
            
            <p>The provided dataset contained not only the required output features (atmospheric pressure, air temperature, relative humidity, wind velocity) but also numerous others including date, time, and H2O content. For our experiments, we chose to provide the models with only the four required features assess their performance while receiving only a limited feature set and to simplify calculations of error propagation (discussed in section 4.1). However, as will be later discussed in section 4.2, we chose to assess the models’ respective abilities to cast predictions on additional features, while still only having the four required features as inputs.</p>
            
            <p>The preprocessing performed was minimal and targeted. After loading the training and test datasets, we normalized both column-wise so each feature was between 0 and 1. This transformation was also applied to the output features so that when it came time to calculate the model’s loss, each feature output was normalized in [0, 1] resulting in equally weighted loss across output values. However, to ensure that practical outputs could be obtained such that each was represented in appropriate units, the scaler used to normalize both inputs and labels was saved so as a reverse transformation could be applied to output values, returning them to their desired units.</p>
            
            <p>It should also be noted that when month and time of day were considered for input feature candidates, a different form of preprocessing was required to address their cyclical natures. Instead of simply normalizing these features with a scaler as above, we first input them to sine and cosine functions such that each represented a smooth sinusoid, whose outputs were in turn normalized on [0, 1] and added to the feature set. The specific preprocessing details for each model are summarized in the following subsections.</p>
            
            <p>LSTM: First, we perform median and gaussian filters on both training and testing datasets. Selected inputs are fed into the model, one by one, for each round of training, therefore, at this step we drop all feature columns that are not desired. Data is normalized from 0 to 1 using MinMaxScaler function and splitted into inputs and labels according to the timestamps declared earlier in the model (k=7).</p>
            
            <p><strong> Model Architecture</strong></p>
            
            <p>Using Keras we construct our model with 3 layers of which two of them are long short-term memory (LSTM) and one Dense layer. Both of the LSTM layers consist of a ’relu’ activation function to overcome the vanishing gradient. In addition to this settings, the first LSTM layer returns the hidden state output for each input time step using return sequences. Therefore, our second LSTM layer returns a 3-D sized Tensor with 50 memory units (smart neurons). We use a Dense output layer with a single neuron, as the final step, to return a 3-D prediction.</p>
            
            <p>Loss Function: For all three models, we elected to use mean squared error (MSE) as our loss function. This was selected as it is a very popular candidate for regression task loss functions [2]. Additionally, since all model outputs were normalized on [0, 1] as mentioned in section 2.2, the mean squared error was calculated as an unbiased mean across all output features and no loss customization was required to ensure that the importance of each output was weighted evenly.</p>
            <p>The LSTM model was trained under the following conditions:</p>
            <p><ul style="float: right; margin-right: 100px;">
                <li>Training samples: 56065</li>
                <li>Batch size: 32</li>
                <li>Epochs: 30</li>
                <li>Optimizer: Adam with B1 = 0.9,B2 = 0.99,Epsilon = 1e − 9</li>
                <li>Learning rate: Initial rate of 0.001 with linear decay and 500 warmup steps</li>
                </ul>
            </p> 
            <p>To make sure that the model learns several patterns based on the historical data provided, we use all the training samples with batches of 32 and 30 numbers of epochs. The numbers chosen are enough to train the model without overfitting. Similar to the rest of the models, we use mean squared error to obtain the loss in the network’s output.</p> 
            <p><strong>Results</strong></p>
            <p>Matrics comparison: To compare model performances, we chose to use the mean squared error across the entire test set, both peroutput and overall. We believe this to be a robust measure of performance as it allows for an understanding of the degree of prediction inaccuracy for each model as well as the distribution of these inaccuracies across outputs.</p>     
            <p>table for MSE and graphs</p>
            <p><strong>Further Analysis</strong></p>
            <p>The pressure and temperature of any gas, including air, are directly proportional according to Frenchman Joseph Gay-Lussac law (1778 to 1850) [6]. This gas law shows that, if the mass and volume of any given sample of gas are held constant, as the sample’s temperature increases, so will its pressure, and vice versa. Mathematically:</p>
            <p>equation</p>
            <p>where kG is the appropriate proportionality constant. The main purpose of this experiment is to verify whether this equality can be satisfied by our extracted results. We have selected the LSTM model to implement this task for us. The assumption is to have a fixed amount of air in a large area outdoors, e.g. a national park, where we can safely keep mass and volume constant.</p>
            <p>To start, we converted the pressure and temperature units of both testing and predicted sets to atm and Kelvin, respectively. Next, we calculated the kG constant, P T ratio, for each dataset, separately. finally, we visualized the outcomes to easily compare our finding with ground truth.</p>
            <p>As you can notice in Fig. 7, there are some spikes in true value graph that have been mostly eliminated during prediction. We believe this might have been caused by the change in atmospheric dynamic (wind blow for example) where the air mass within a certain volume is no longer fixed. However, in general, our result is perfectly synchronized with the real data. The P T ratio follows a constant value close to zero (both pressure and temperature values are normalized between 0 and 1). Our finding shows that we not only achieved a constant kG throughout the entire dataset but also its magnitude is very close to the kG of the real data with a minimal MSE of 1.08444e-05. This indicates our LSTM model is able to memorize long patterns and predict atmospheric features as close to actual data as possible.</p>
        
        
        </div>
        
        
        
        <!-- Footer -->
        <footer class="footer">
            <a class="footer__link" href="https://drive.google.com/file/d/1iTq45jmv4arTfgiiFCktuzWh-b7ZC96u/view?usp=sharing" target="_blank" >Curriculum Vitae</a>
            <ul class="social-list">
               
                <li class="social-list__item">
                    <a class="social-list__link" href="https://www.linkedin.com/in/tooba-rahimnia/" target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li class="social-list__item">
                    <a class="social-list__link" href="https://github.com/toobarahimnia" target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
            </ul>
            <p>Created by Tooba Rahimnia © 2021</p>
        </footer>
        
        
        <script src="js/index.js"></script>
        
    </body>
</html>